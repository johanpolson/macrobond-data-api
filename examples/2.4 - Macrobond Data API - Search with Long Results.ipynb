{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - Macrobond Data API for Python - Search with Long Results\n",
    "\n",
    "*Retrieving large list of time series based on search parameters*\n",
    "\n",
    "This notebook aims to provide examples of how to use Macrobond Data API for Python as well as insights on the key attributes used to display the output in an understandable format.\n",
    "\n",
    "We will focus on using the Search method based on a few inputs e.g. **Source, Frequency, Category and Region**. This helps you download large list of time series when you start building your universe. \n",
    "The emphasis will be on function **'entity_search_multi_filter_long' to pull up to 240,000 time series in the response instead of up to 12,000 using search_entites or entity_search_multi_filter**. In case your response should pull back more than 240,000 entities, we recommend to further narrow down the search using attributes such as Region, Frequency or Category.\n",
    "\n",
    "You can find a full description of all methods and parameters used in the examples in the [documentation of the API](https://macrobond.github.io/macrobond-data-api/common/api.html).\n",
    "\n",
    "*Full error handling is omitted for brevity*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import macrobond_data_api as mda\n",
    "from macrobond_data_api.web import WebClient\n",
    "from macrobond_data_api.common.types import SearchFilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Searching time series with wide search parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical Search requests with metadata being fetched can download a maximum of 12,000 entities. With the `entity_search_multi_filter_long()` function, you can retrieve up to 240,000 entities in a single request.\n",
    "\n",
    "Note that there is no pagination in the response and if your response contains more than 240,000 entities, we recommend you narrow down the search parameters further.\n",
    "\n",
    "In the example below, we will download the list of time series codes coming from source: \"Tourism Research Australia\". The code of the data source used in this request is: `src_autra`.\n",
    "\n",
    "Feel free to use other search parameters. You can find examples in notebooks 2.2 - Filter on a Concept and 2.3 - Filter with a Keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "mda.get_one_entity(\"src_autra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "with WebClient() as api:\n",
    "    ids = api.entity_search_multi_filter_long(\n",
    "        SearchFilter(entity_types=\"TimeSeries\", must_have_values={\"source\": \"src_autra\"}),\n",
    "        include_discontinued=False,\n",
    "    ).entities\n",
    "\n",
    "print(\"Number of time series in universe:\", len(ids))\n",
    "ids[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now download the list of time series codes coming from this source."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Fetching the data from the Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated our universe, we are going to fetch the date, value and metadata on each of these time series.\n",
    "\n",
    "As industry standard, we recommend our users to download our data by chunks (200 entities each chunk will be ideal) when the universe is large, using `get_entites` and `get_series` methods. \n",
    "\n",
    "Alternatively, we also provide convenient solution: our method `get_many_series` can deal with large universe and download the data in one single request, since `get_many_series` handles batches by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "payload_ids = [(id, None) for id in ids]\n",
    "\n",
    "for x in mda.get_many_series(*payload_ids):\n",
    "    df = pd.concat([df, pd.json_normalize([x.to_dict()])])\n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    df,\n",
    "    columns=[\n",
    "        \"metadata.Name\",\n",
    "        \"metadata.LastModifiedTimeStamp\",\n",
    "        \"metadata.StartDate\",\n",
    "        \"metadata.LastValueDate\",\n",
    "        \"metadata.Frequency\",\n",
    "        \"metadata.FullDescription\",\n",
    "    ],\n",
    ")\n",
    "df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c65b52ed67bf21a68d91eaf7359cae5864732179fe6cfc221f72bfe728ed10c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
