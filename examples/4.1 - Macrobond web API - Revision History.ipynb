{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 - Macrobond web API - Revision History\n",
    "\n",
    "*Using Macrobond's web API features to query one or more time series in a point-in-time fashion. Revision history is defined as the feature allowing to request the original values of a time series before it could have been revised by the source. The vintageTimeStamp provides the point-in-time stamp as of which the series was known to have the values retrieved. This time stamp corresponds to when Macrobond has made the data available in its database.*\n",
    "\n",
    "This notebook aims to provide examples of how to use Macrobond's web API call methods to work with Revision History.\n",
    "\n",
    "We will focus here on using various calls from observing whether a time series carries Revision History up to requesting the full array of historical revisions.\n",
    "\n",
    "*The examples uses the Web API, but you can chose to use the desktop COM API and get the same result. Full error handling is omitted for brevity*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from oauthlib.oauth2 import BackendApplicationClient\n",
    "# from requests_oauthlib import OAuth2Session\n",
    "# import json\n",
    "# import matplotlib.ticker as tck\n",
    "# import pandas as pd\n",
    "# import seaborn\n",
    "# import plotly.express as px\n",
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.graph_objects as go\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.dates as md\n",
    "# import getpass as gb\n",
    "# import numpy as np\n",
    "# import datetime as dt\n",
    "# import time\n",
    "\n",
    "\n",
    "import pandas\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from macrobond_financial.web import WebClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Get the data - GetRevisionInfo\n",
    "\n",
    "GetRevisionInfo provides high level information whether the time series store Revision History and had Revisions yet or not. The output also provides the list of vintageTimeStamps for series carrying Revision History."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with WebClient() as api:\n",
    "    data_frame = api.get_revision_info(\"gbgdp\").to_pd_data_frame()\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our time series is enabled to store Revision History and already had revisions. Our records start in 2016-03-31.\n",
    "Let's now observe the most recent time stamps recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with WebClient() as api:\n",
    "    info = api.get_revision_info(\"gbgdp\").object()[0]\n",
    "list(map(str,info.vintage_time_stamps))[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Get the data - FetchObservationHistory\n",
    "\n",
    "FetchObservationHistory finds the various values a specific historical release 'observationDate' could have had over time. The historical revisions are provided with their respective dates of integration 'timeStamps'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadMbApi(request):\n",
    "    url = \"https://api.macrobondfinancial.com/v1/\" + request\n",
    "    # This is typically the first time when we do not yet have a token\n",
    "    if not mbapi.authorized:\n",
    "        mbapi.fetch_token(\n",
    "            token_url=token_url, client_id=client_id, client_secret=client_secret\n",
    "        )\n",
    "    r = mbapi.get(url)\n",
    "    if r.status_code != 401:\n",
    "        return r\n",
    "    # If authorization failed, it is likely that the token has expired. Get a new one and try again.\n",
    "    mbapi.fetch_token(\n",
    "        token_url=token_url, client_id=client_id, client_secret=client_secret\n",
    "    )\n",
    "    r = mbapi.get(url)\n",
    "    return r\n",
    "\n",
    "\n",
    "# We are fetching here the Q3 2020 observations for gbgdp (United Kingdom GDP).\n",
    "d = downloadMbApi(\"series/fetchobservationhistory?n=gbgdp&t=2020-07-01\")\n",
    "x = d.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame((x), columns=[\"values\", \"timeStamps\"])\n",
    "df3 = df3.to_numpy().flatten()\n",
    "data = pd.DataFrame()\n",
    "for i in df3:\n",
    "    df3 = pd.DataFrame(i)\n",
    "    data = pd.concat([data, df3], axis=1)\n",
    "data.columns = [\"values\", \"timeStamps\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review TO: Do we really want to quote specific observations and values here? The underlying data will change and the text will be wrong.**\n",
    "As we have requested here the historical value of GB GDP accounting for Q3 2020, we can see that it was originally published on 2021-02-12 at 7AM UTC with a value of 490,861,000,000.\n",
    "This Q3 2020 GB GDP was then further revised 3 times with the latest revision to date published on 2021-05-20 at 8.32AM UTC with a value of 498,429,000,000.\n",
    "\n",
    "Let's measure the time difference from the first publication to the last revision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.timeStamps = pd.to_datetime(data.timeStamps)\n",
    "data.timeStamps.max() - data.timeStamps.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [16, 9]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    data[\"timeStamps\"],\n",
    "    data[\"values\"],\n",
    "    color=(27 / 255, 54 / 255, 93 / 255),\n",
    "    linewidth=4,\n",
    ")\n",
    "plt.title(\"Q3 2020 GB GDP historical revisions\", fontsize=24)\n",
    "plt.xlabel(\"Vintages\", fontsize=14)\n",
    "plt.ylabel(\"GB GDP\", fontsize=14)\n",
    "ax.grid()\n",
    "plt.autoscale()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Get the data - FetchNthReleaseSeries\n",
    "\n",
    "FetchNthReleaseSeries retrieves the nth change of value of the requested time series.\n",
    "Now that we have pulled out the revisions of a specific time series for a fixed observation via the FetchObservationHistory endpoint, we can isloate the nth revision in this array. This can be particularly useful to observe patterns of revisions from a single source or on a unique concept across various regions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are fetching here the first revision (n=1) for each observation of gbgdp (United Kingdom GDP).\n",
    "d = downloadMbApi(\"series/fetchnthreleaseseries?nth=1&getTimesOfChange=true&n=gbgdp\")\n",
    "x = d.json()\n",
    "\n",
    "with WebClient(credentials.client_id, credentials.client_secret) as api:\n",
    "    info = api.revision.get_revision_info(\"gbgdp\").object()[0]\n",
    "list(map(str,info.vintage_time_stamps))[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are fetching the first revision for each observation, let's bear in mind that the very first vintage recorded for this series was on 2016-03-31 i.e. there was no revision recorded beforehand hence we can expect to see no timesOfChange nor values prior to this date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame((x), columns=[\"timesOfChange\", \"dates\", \"values\"])\n",
    "df4 = df4.to_numpy().flatten()\n",
    "data = pd.DataFrame()\n",
    "for i in df4:\n",
    "    df4 = pd.DataFrame(i)\n",
    "    data = pd.concat([data, df4], axis=1)\n",
    "data.columns = [\"timesOfChange\", \"dates\", \"values\"]\n",
    "data.dates = pd.to_datetime(data.dates, dayfirst=True)\n",
    "data.sort_values(by=\"dates\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review TO: Do we really want to quote specific observations and values here? New values will be added to this series, and value 262 will no longer be listed above.**\n",
    "\n",
    "We observe here on row 262 the timeOfChange = 2021-03-31T06:00:00Z and value 497,909,000,000 that we previously returned when using FetchObservationHistory on Q3 2020 for this series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Get the data - FetchVintageSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are fetching here the the gbgdp (United Kingdom GDP) as of 2020-06-30, which is the vintage we have been referring to before.\n",
    "d = downloadMbApi(\n",
    "    \"series/fetchvintageseries?t=2020-07-01&getTimesOfChange=false&n=gbgdp\"\n",
    ")\n",
    "x = d.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have inserted the time of the vintage to retrieve in the query = 2020-07-01 which is one day after the actual vintage in order to make sure we fall back on the most recent one prior to this date, which is 2020-06-30. We also could have been using a time stamp just after 06:00:00Z as this is when the vintage was recorded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame((x), columns=[\"vintageTimeStamp\"])\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now observe the time series as it was as of that date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame((x), columns=[\"dates\", \"values\"])\n",
    "df6 = df6.to_numpy().flatten()\n",
    "data_pit = pd.DataFrame()\n",
    "for i in df6:\n",
    "    df6 = pd.DataFrame(i)\n",
    "    data_pit = pd.concat([data_pit, df6], axis=1)\n",
    "data_pit.columns = [\"dates\", \"values\"]\n",
    "data_pit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot our time series as of this specific vintage time stamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_h = pd.to_datetime(data_pit.dates)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(dates_h, data_pit[\"values\"], color=(27 / 255, 54 / 255, 93 / 255), linewidth=4)\n",
    "plt.title(\"GB GDP as of 2020-06-30\", fontsize=24)\n",
    "plt.xlabel(\"Dates\", fontsize=14)\n",
    "plt.ylabel(\"GB GDP\", fontsize=14)\n",
    "ax.grid()\n",
    "plt.autoscale()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Get the data - FetchAllVintageSeries\n",
    "\n",
    "FetchAllVintageSeries retrieves the full array of revisions over time for the requested time series.\n",
    "We can build a dataframe of such revisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are fetching here the the gbgdp (United Kingdom GDP) as of 2020-06-30, which is the vintage we have been referring to before.\n",
    "d = downloadMbApi(\"series/fetchallvintageseries?n=gbgdp\")\n",
    "x = d.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(x)\n",
    "s = data\n",
    "data.vintageTimeStamp = pd.to_datetime(data.vintageTimeStamp, dayfirst=True)\n",
    "\n",
    "\n",
    "vintageTimeStamp = data[\"vintageTimeStamp\"].values\n",
    "data = data.drop([\"metadata\", \"vintageTimeStamp\"], axis=1)\n",
    "df1 = data.to_numpy().flatten()\n",
    "# ----------------------------------------\n",
    "df = pd.DataFrame()\n",
    "for i in df1:\n",
    "    df2 = pd.DataFrame(i)\n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "i = 1\n",
    "sn = []\n",
    "for j in range(len(df.columns) // 2):\n",
    "    series1 = pd.DataFrame(df.iloc[:, i - 1])\n",
    "    series2 = pd.DataFrame(df.iloc[:, i])\n",
    "    sn1 = pd.concat([series1, series2], axis=1)\n",
    "    l = str(vintageTimeStamp[(i - 1) // (2)])\n",
    "    sn1.columns = [\"dates\", l]\n",
    "    sn1 = sn1.set_index(\"dates\")\n",
    "    sn1.index.name = None\n",
    "    sn1 = sn1.squeeze()\n",
    "    sn1 = sn1[np.logical_not(np.isnan(sn1))]\n",
    "    sn.append(sn1)\n",
    "    i = i + 2\n",
    "\n",
    "dff = pd.DataFrame(sn)\n",
    "data = dff.T\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2881cb102472c0a2cc7b98c9f5b468e81049b20ea6d80c54eb33ba6f13a3277c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f07703081b087c011f489eea73b431cab9a444b9d10d39f2f152d47c6ed00d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
